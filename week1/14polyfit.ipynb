{
   "nbformat" : 4,
   "cells" : [
      {
         "cell_type" : "markdown",
         "source" : [
            "## Producing some fake data\n\n"
         ],
         "metadata" : {}
      },
      {
         "metadata" : {},
         "source" : [
            "Let&rsquo;s use `numpy` to produce some fake data.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "import numpy as np\n\nN = 100\nxs = np.random.uniform(0,np.pi,N)\nxs = np.sort(xs) # for plotting later\nys = np.sin(xs) + np.random.normal(0,0.1,N)"
         ],
         "metadata" : {},
         "execution_count" : 1,
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Note that `xs` and `ys` are not Python lists but rather `numpy`\narrays.  For &ldquo;technical reasons&rdquo; we&rsquo;d prefer to restructure the arrays\nas follows.\n\n"
         ]
      },
      {
         "source" : [
            "xs = xs[:, np.newaxis]\nys = ys[:, np.newaxis]"
         ],
         "metadata" : {},
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Let&rsquo;s see a plot of our random data.\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "cell_type" : "code",
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "import matplotlib.pyplot as plt\nplt.scatter(xs,ys)\nplt.show()"
         ]
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "We now pretend that we don&rsquo;t know the source of this data, and we wish\nto &ldquo;learn&rdquo; the relationship between the $x$&rsquo;s and the $y$&rsquo;s.  Of\ncourse, the truth is that $y = \\sin x$ plus some noise, but let&rsquo;s\nforget about that and see what we can recover.\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## Linear regression\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "We&rsquo;ve been learning about linear regression, so let&rsquo;s use\n`scikit-learn` to **again** perform linear regression on our data.\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "cell_type" : "code",
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "from sklearn.linear_model import LinearRegression\nlm = LinearRegression().fit( xs, ys )"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "Let&rsquo;s make a plot our (linear!) model.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "ys_predicted = lm.predict(xs)\nplt.scatter(xs, ys)\nplt.plot(xs, ys_predicted, color='r')\nplt.show()"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Because we followed the usual advice to ****look at our data****, we know\nour data isn&rsquo;t modeled well by a straight line.  This is an example of\n**underfitting**.  We need a more complex model to capture the actual\npattern of the data.\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Use polynomials\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "We find the &ldquo;polynomial features&rdquo; associated to the $x$&rsquo;s.  This\nreplaces the vector $(x) \\in \\mathbb{R}^1$ with the vector $(1,x,x^2)\n\\in \\mathbb{R}^3$.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "from sklearn.preprocessing import PolynomialFeatures\npolynomial_features= PolynomialFeatures(degree=2)\nxs_poly = polynomial_features.fit_transform(xs)"
         ],
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1
      },
      {
         "source" : [
            "Next we perform linear regression.  A ****common misconception**** would\nbe that **linear** regression is the wrong choice since we want to find\nan approximating polynomial.  Can you explain why we are nevertheless\nusing `LinearRegression` in an attempt to find a polynomial fit?\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "qm = LinearRegression().fit( xs_poly, ys )"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "Let&rsquo;s plot the data as a scatterplot, and our model&rsquo;s predicted values\nas a red curve.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "code",
         "outputs" : [],
         "execution_count" : 1,
         "source" : [
            "ys_predicted = qm.predict(xs_poly)\nplt.scatter(xs, ys)\nplt.plot(xs, ys_predicted, color='r')\nplt.show()"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "That **looks** much better.  Is it &ldquo;actually&rdquo; better?\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1,
         "metadata" : {},
         "source" : [
            "print(\"linear model score:\",lm.score(xs,ys))\nprint(\"quadratic model score:\",qm.score(xs_poly,ys))"
         ]
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Would a cubic polynomial be an even better choice?\n\n"
         ],
         "metadata" : {}
      },
      {
         "metadata" : {},
         "source" : [
            "## Overfitting\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "If degree 2 worked well, surely degree 25 is even better!\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "from sklearn.preprocessing import PolynomialFeatures\npolynomial_features= PolynomialFeatures(degree=25)\nxs_poly = polynomial_features.fit_transform(xs)\n\nlm = LinearRegression().fit( xs_poly, ys )\n\nys_predicted = lm.predict(xs_poly)\nplt.scatter(xs, ys)\nplt.plot(xs, ys_predicted, color='r')\nplt.show()"
         ],
         "metadata" : {},
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1
      },
      {
         "source" : [
            "Our prediction now is rather wiggly; what we are doing is not so much\nlearning the regularities in the data as we are **learning the noise**.\nHow could we possibly deal with this?  What sort of framework could\nhelp us pick &ldquo;hyperparameters&rdquo; like degree?  These questions pave the\nway for Week 2, when we study the **bias/variance tradeoff**.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      }
   ],
   "metadata" : {
      "kernelspec" : {
         "display_name" : "Python 3",
         "name" : "python3",
         "language" : "python"
      },
      "org" : null,
      "language_info" : {
         "codemirror_mode" : {
            "version" : 3,
            "name" : "ipython"
         },
         "nbconvert_exporter" : "python",
         "name" : "python",
         "version" : "3.5.2",
         "pygments_lexer" : "ipython3",
         "file_extension" : ".py",
         "mimetype" : "text/x-python"
      }
   },
   "nbformat_minor" : 0
}
