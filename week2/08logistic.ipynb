{
   "metadata" : {
      "language_info" : {
         "version" : "3.5.2",
         "nbconvert_exporter" : "python",
         "pygments_lexer" : "ipython3",
         "name" : "python",
         "file_extension" : ".py",
         "codemirror_mode" : {
            "name" : "ipython",
            "version" : 3
         },
         "mimetype" : "text/x-python"
      },
      "org" : null,
      "kernelspec" : {
         "language" : "python",
         "name" : "python3",
         "display_name" : "Python 3"
      }
   },
   "nbformat_minor" : 0,
   "nbformat" : 4,
   "cells" : [
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "We&rsquo;ve previously done some regression (to predict quantities) and some\nclassification (to predict a label).  But sometimes what we really\nwant is to phrase our predictions as **probabilities**.\n\nAs usual, each $x$ is is a vector and the corresponding $y \\in \\{ 0, 1\n\\}$ is the desired label.  On an unseen $x$, our model should produce\na **probability** that the corresponding $y$ is $0$ or $1$.  We write\n$p(x)$ for this model.  In other words, we are building a **stochastic\nmodel** because the outputs are probabilities rather than a specific\nclassification.  We can still produce a classifier from our stochastic\nmodel: if the model outputs a probability $> 1/2$ then we output $1$,\nand if the model outputs a probability $< 1/2$ then we output $0$.\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "## The logistic function\n\n"
         ]
      },
      {
         "source" : [
            "For us, &ldquo;sigmoid function&rdquo; refers to the logistic function $S$ defined by\n$S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}$.  \n\nYou might define this by\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "source" : [
            "import math\ndef S(x):\n    return 1/(1 + math.exp(-x))"
         ],
         "metadata" : {},
         "cell_type" : "code"
      },
      {
         "source" : [
            "Plot it!\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "cell_type" : "code",
         "metadata" : {},
         "outputs" : [],
         "source" : [
            "import matplotlib.pyplot as plt\nraise Exception(\"Use matplotlib to plot S\")"
         ],
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "What is the image of $S$?  You&rsquo;ll be happy computing this.\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "S(-709)"
         ],
         "outputs" : []
      },
      {
         "source" : [
            "You&rsquo;ll be sad when you compute this.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "cell_type" : "code",
         "execution_count" : 1,
         "outputs" : [],
         "source" : [
            "S(-710)"
         ]
      },
      {
         "source" : [
            "What happened?  One solution is the following.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "source" : [
            "def S(x):\n    if x < 0:\n        return math.exp(x) / (1 + math.exp(x)) \n    else:\n        return 1 / (1 + math.exp(-x))"
         ],
         "metadata" : {},
         "cell_type" : "code"
      },
      {
         "source" : [
            "Another choice is to invoke `scipy` which provides an `expit` function\ndefined on arrays.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "source" : [
            "import scipy\nfrom scipy.special import expit\n\nprint(expit(range(-10,10)))"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "source" : [
            "This `S` or `expit` function is the inverse to the `logit` function,\ndefined as follows.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "source" : [
            "def logit(p):\n    return math.log( p / (1 - p) )"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "What&rsquo;s the domain of `logit`?  Plot it.\n\n"
         ]
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "source" : [
            "import matplotlib.pyplot as plt\nraise Exception(\"Use matplotlib to plot logit\")"
         ],
         "metadata" : {},
         "cell_type" : "code"
      },
      {
         "source" : [
            "You can also find `logit` in `scipy`, and the `logit` defined there is\nalso valid for arrays.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "source" : [
            "from scipy.special import logit"
         ],
         "outputs" : [],
         "metadata" : {},
         "cell_type" : "code"
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "What happens if you compose `logit` and `expit`?\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "code",
         "execution_count" : 1,
         "outputs" : [],
         "source" : [
            "logit(expit(17))"
         ]
      },
      {
         "outputs" : [],
         "source" : [
            "expit(logit(0.17))"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "## The logistic model\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "Armed with `logit` and `expit`, we relate $\\mathbb{R}$ to $$0,1)$.\n\nIf we were to model the probability directly, we might do something\nsilly like report probabilities larger than 1 or report negative\nprobabilities, but now we can model `logit` of the probability, and\n`logit` is unbounded.  This `logit` is said to be the &ldquo;log odds&rdquo;\nbecause it is the logarithm of the odds.  Any real number is a\nreasonable choice for the log odds; if we want the probability, we\ntake `expit` of the log odds.\n\nAssuming the data is drawn from $\\mathbb{R}^n$, our model has\nparameters $\\beta_1, \\ldots, \\beta_n$ along with an intercept\n$\\beta_0$, and the model reports $p(x) = S(\\sum \\beta_i x_i +\n\\beta_0)$.  Since $S(0) = 0.5$, the classification problem boils down\nto whether or not $\\sum \\beta_i x_i + \\beta_0$ is positive or negative.\n\n**This is just a model.** Nobody is promising that this is going to fit\nthe data all that well, but it provides a way, with $n+1$ parameters,\nto convert $x$&rsquo;s into a number between $0$ and $1$, which we can\nregard as a probability.  What would it even mean for the logistic\nmodel to report the &ldquo;true&rdquo; probabilities?\n\n"
         ]
      },
      {
         "source" : [
            "## Logistic regression\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "source" : [
            "You should not be surprised that it isn&rsquo;t hard to perform logistic regression with `scikit-learn`.  Let&rsquo;s load the iris data from your homework.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "outputs" : [],
         "source" : [
            "from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nX, y = load_iris(return_X_y=True)"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "Split the data into a training and a testing set.\n\n"
         ]
      },
      {
         "source" : [
            "from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y)"
         ],
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "Train the model.\n\n"
         ]
      },
      {
         "outputs" : [],
         "source" : [
            "model = LogisticRegression(solver='lbfgs',multi_class='multinomial')\nmodel.fit(X_train, y_train)"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "How did it do?\n\n"
         ]
      },
      {
         "cell_type" : "code",
         "metadata" : {},
         "outputs" : [],
         "source" : [
            "from sklearn import metrics\npredicted = model.predict(X_test)\nmetrics.classification_report(y_test, predicted)\nprint(metrics.confusion_matrix(y_test, predicted))"
         ],
         "execution_count" : 1
      },
      {
         "source" : [
            "What do these terms mean?  You might remember (or not remember)\n&ldquo;sensitivity and specificity&rdquo; from medical tests.\n\n****Precision**** is the ratio of correctly predicted observations (in some\nclass) to the total number of observations predicted in the class.\nThink &ldquo;low false positive.&rdquo;  Or think &ldquo;out of everyone **predicted** to\nwin, how many won?&rdquo;\n\n****Recall**** is the ratio of correctly predicted observations (in some\nclass) to the total number of observations labeled as such.  Think\n&ldquo;out of everyone who won, how many did we predict to win?&rdquo;\n\nHow do these relate to the confusion matrix?  The difference is\nwhether we&rsquo;re summing over rows or columns.\n\nThen the ****F1 score**** is the weighted average of precision and recall.\nIt&rsquo;s a weighted average, but remember that you may worry more (or\nless) about false negatives than about false postives.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      }
   ]
}
