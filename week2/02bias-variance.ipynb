{
   "nbformat" : 4,
   "nbformat_minor" : 0,
   "metadata" : {
      "language_info" : {
         "file_extension" : ".py",
         "mimetype" : "text/x-python",
         "version" : "3.5.2",
         "name" : "python",
         "nbconvert_exporter" : "python",
         "pygments_lexer" : "ipython3",
         "codemirror_mode" : {
            "name" : "ipython",
            "version" : 3
         }
      },
      "org" : null,
      "kernelspec" : {
         "display_name" : "Python 3",
         "language" : "python",
         "name" : "python3"
      }
   },
   "cells" : [
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## What does learning mean?\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "What does &ldquo;learning&rdquo; even mean?  At a given $x$, what we **measure** is\n$y = f(x) + \\epsilon$ where $f$ is some (deterministic!) function and\n$\\epsilon$ is noise with mean 0 and variance $\\sigma^2$.  By\nperforming many measurements, we get many pairs $(x_i,y_i)$.  Via some\nsupervised learning algorithm, we produce a function $\\hat{f}$.\n\nHopefully $(y - \\hat{f}(x))^2$ is small.  If we succeed in producing\nsuch an $\\hat{f}$, we might say &ldquo;we have learned $f$.&rdquo;\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Bias and variance\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "We define bias by $\\mathrm{Bias}[\\hat{f}(x)] =\n\\mathbb{E}[\\hat{f}(x)] - f(x)$.  This is the difference between the\nexpected prediction of our model and the truth.\n\nWe define variance by $\\mathrm{Var}[\\hat{f}(x)] =\n\\mathbb{E}[\\hat{f}(x)^2] - \\mathbb{E}[\\hat{f}(x)]^2$.  This is\ncapturing the variability of the model prediction.\n\nThen just **from algebra** we note that, for a previously unseen sample $x$, $\\mathbb{E}\\left[ (y -\n\\hat{f}(x))^2 \\right] = \\left( \\mathrm{Bias}[\\hat{f}(x)] \\right)^2 +\n\\mathrm{Var}[\\hat{f}(x)] + \\sigma^2$.  But $\\mathbb{E}\\left[ (y -\n\\hat{f}(x))^2 \\right]$ is the expected value of the squared error\nbetween what we see ($y$) and what our model predicts ($\\hat{f}(x)$),\nand the algebra then shows this to be the sum of three nonnegative\nterms.  The noise in our data (with variance $\\sigma^2$) provides a\nlower bound for the expected error on unseen data.\n\nThe consequence is a ****bias-variance tradeoff****.  There are low-bias,\nhigh-variance models like k-nearest neighbors; there are high-bias,\nlow-variance models like linear regression.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Produce some fake data to see this\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "We recall how we handled this back for polynomial fitting.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code",
         "metadata" : {},
         "source" : [
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nN = 15\n\nf = np.sin\nmin_x = 0\nmax_x = 3*np.pi\n\ndef noise():\n    return np.random.normal(0,0.5,N)\n\ndef some_data():\n    xs = np.random.uniform(min_x, max_x,N)\n    xs = np.sort(xs)\n    ys = f(xs) + noise()\n    xs = xs.reshape(N,1)\n    ys = ys.reshape(N,1)\n    return xs, ys"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "Earlier we said $y = f(x) + \\epsilon$, and here we wrote `ys = f(xs) + noise()`.  \n\nAs always, we ****look at our data**** before continuing.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "xs, ys = some_data()\n\nimport matplotlib.pyplot as plt\nplt.scatter(xs,ys)\nplt.show()"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Let&rsquo;s pick a test point `x_test`.\n\n"
         ]
      },
      {
         "execution_count" : 1,
         "cell_type" : "code",
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "x_test = (max_x + min_x)/2.0\n\ndef y_test():\n    return f(x_test) + noise()[0]"
         ]
      },
      {
         "metadata" : {},
         "source" : [
            "## The model\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "We said that we&rsquo;d produce a function $\\hat{f}$.  Let&rsquo;s do that now.\nIn this case, $\\hat{f}$ will be denoted `fhat(d,x)` where `d` refers\nto the degree of the polynomial approximation.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef fhat(d, x):\n    xs, ys = some_data()\n\n    polynomial_features = PolynomialFeatures(degree=d)\n    xs_poly = polynomial_features.fit_transform(xs)\n    model = LinearRegression().fit( xs_poly, ys )\n\n    x_poly = polynomial_features.fit_transform([[x]])\n    y_predicted = model.predict( x_poly )\n\n    return y_predicted"
         ],
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code"
      },
      {
         "metadata" : {},
         "source" : [
            "Note that `fhat` is stochastic.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Expectation, in Python\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "To make the Python look more like &ldquo;mathematics,&rdquo; define expected value\nas follows.\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "def expectation(g):\n    return np.mean( [g() for _ in range(1000) ] )"
         ],
         "metadata" : {},
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1
      },
      {
         "source" : [
            "Then we can write code like this.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "expectation(lambda: np.random.normal(17,1) )"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Hopefully $(y - \\hat{f}(x))^2$ is small.  If we succeed in producing\nsuch an $\\hat{f}$, we say that we&rsquo;ve learned $f$.\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Bias and variance\n\n"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "We want to control $\\mathbb{E}\\left[ (y - \\hat{f}(x))^2 \\right]$.\nUsing a degree $d$ model, we denote this quantity by `error(d)`.\n\n"
         ],
         "metadata" : {}
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "def error(d):\n    return expectation( lambda: (y_test() - fhat(d,x_test)) ** 2 )"
         ],
         "metadata" : {}
      },
      {
         "metadata" : {},
         "source" : [
            "Recall that $\\mathrm{Bias}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)] - f(x)$.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "def bias(d):\n    return (expectation( lambda: fhat(d,x_test) ) - f(x_test) )"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Recall $\\mathrm{Var}[\\hat{f}(x)] = \\mathbb{E}[\\hat{f}(x)^2] - \\mathbb{E}[\\hat{f}(x)]^2$.\n\n"
         ]
      },
      {
         "cell_type" : "code",
         "outputs" : [],
         "execution_count" : 1,
         "source" : [
            "def variance(d):\n    return expectation( lambda: fhat(d,x_test) ** 2 ) - expectation( lambda: fhat(d,x_test) ) ** 2"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Then we can see how bias and variance are affected by degree, which\nplays the role of model complexity.\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "outputs" : [],
         "cell_type" : "code",
         "metadata" : {},
         "source" : [
            "degrees = range(1,9)\nplt.plot(degrees, [bias(d)**2 for d in degrees],label='squared bias')\nplt.plot(degrees, [variance(d) for d in degrees],label='variance')\nplt.plot(degrees, [error(d) for d in degrees], label='error')\nplt.legend()\nplt.xlabel('degree')\nplt.ylabel('error')\nplt.show()"
         ]
      }
   ]
}
