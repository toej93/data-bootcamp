{
   "cells" : [
      {
         "source" : [
            "A **cost function** is sometimes called a **loss** function.  Speaking\nvery generally, this is some quantity to be minimized.  In our\nspecific context, the &ldquo;cost&rdquo; is a measure of the wrongness of the\nmodel, and we want to wiggle the parameters to minimize this\nwrongness.\n\nBecause we want to get a look inside the black boxes, we&rsquo;ll play with\nthis idea just using `numpy` and **without** using the whole power of\n`scikit-learn`.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "cell_type" : "code",
         "metadata" : {},
         "execution_count" : 1,
         "source" : [
            "import numpy as np\nimport matplotlib.pyplot as plt"
         ],
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## The fake data\n\n"
         ]
      },
      {
         "source" : [
            "Let&rsquo;s generate some fake data.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "cell_type" : "code",
         "source" : [
            "N = 100\noriginal_m = m = np.random.normal(1,2)\noriginal_b = b = np.random.normal(1,2)\nX = np.random.uniform(5,10,N)\ny = m * X + b + np.random.normal(0,0.1,N)\n\nplt.scatter(X,y)\nplt.show()"
         ],
         "execution_count" : 1,
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "As we&rsquo;ve rigged it, these `N` data points are near the line `y = mx + b`.\n\nLet&rsquo;s forget that we know about `m` and `b`, and try to **deduce** `m`\nand `b` from the data `X` and `y`.\n\n"
         ]
      },
      {
         "metadata" : {},
         "cell_type" : "markdown",
         "source" : [
            "## The cost function\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Given parameters `m` and `b`, the average of the squared errors can\nserve as our cost function.\n\n"
         ]
      },
      {
         "cell_type" : "code",
         "metadata" : {},
         "execution_count" : 1,
         "source" : [
            "def cost(m,b):\n    return np.mean( (y - (m * X + b)) ** 2 )"
         ],
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Note that even our chosen value of `m` and `b` doesn&rsquo;t make\n`cost(m,b)` vanish, because there is some noise in the data `y`.\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## Minimizing cost\n\n"
         ]
      },
      {
         "source" : [
            "In this case, we have **least squares** available to us to minimize the\ncost function.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "code",
         "metadata" : {},
         "outputs" : [],
         "execution_count" : 1,
         "source" : [
            "(best_m, best_b) = np.linalg.lstsq([[x,1] for x in X], y)[0]\nprint(cost(original_m,original_b))\nprint(cost(best_m,best_b))\ngoal = cost(best_m,best_b) + 0.001"
         ]
      },
      {
         "source" : [
            "## Stochastic gradient descent\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "source" : [
            "If we are standing at $(m,b)$, we could likely reduce $Q$ by moving in\nthe direction of negative gradient.  So we subtract off the a small\nmultiple of the gradient of $Q$; this multiple is $\\eta$, the\n**learning rate**.  We let $\\eta$ decay as we repeat this procedure of\n**gradient descent**.\n\nNote that `cost` has a special structure, namely it is the average $Q$\nof cost functions $Q_i$ associated to the $i$th data point.  That is,\n$Q(m,b)={\\frac {1}{N}}\\sum_{i=1}^{N}Q_{i}(m,b)$.  Often $N$ is very\nlarge and computing $\\nabla Q_i$ could be expensive, so we shuffle the\nindices and only subtract off a few $\\nabla Q_i$&rsquo;s.  This is\n**stochastic** gradient descent.\n\nFor this example, let&rsquo;s just subtract one of the $Q_i$ by randomly\nchoosing an index with `np.random.randint(0,len(X))`.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "cell_type" : "code",
         "metadata" : {},
         "execution_count" : 1,
         "source" : [
            "def sgd_step(m,b,eta):\n    i = np.random.randint(0,len(X))\n    gradm = 2*X[i]*(m*X[i] + b - y[i])\n    gradb = 2*(m*X[i] + b - y[i])\n    return ( m - eta * gradm, b - eta * gradb )"
         ],
         "outputs" : []
      },
      {
         "source" : [
            "We pick random values of `m` and `b`, and then repeat this `sgd_step`\nuntil our `cost` function is small enough.  As we do so, the learning\nrate decays.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "source" : [
            "def sgd():\n    m, b = np.random.normal(size=2)\n    t = 1\n    while cost(m,b) > goal:\n        eta = 0.01 * (0.99 ** (t / N))\n        t = t + 1\n        m, b = sgd_step(m, b, eta)\n        print( cost(m,b) )\n    return m, b\n\nsgd()"
         ],
         "execution_count" : 1,
         "cell_type" : "code",
         "metadata" : {}
      }
   ],
   "nbformat" : 4,
   "nbformat_minor" : 0,
   "metadata" : {
      "kernelspec" : {
         "language" : "python",
         "display_name" : "Python 3",
         "name" : "python3"
      },
      "org" : null,
      "language_info" : {
         "codemirror_mode" : {
            "name" : "ipython",
            "version" : 3
         },
         "nbconvert_exporter" : "python",
         "mimetype" : "text/x-python",
         "name" : "python",
         "file_extension" : ".py",
         "pygments_lexer" : "ipython3",
         "version" : "3.5.2"
      }
   }
}
