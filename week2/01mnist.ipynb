{
   "cells" : [
      {
         "metadata" : {},
         "source" : [
            "## The MNIST dataset\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "You may not know that `scikit-learn` includes a collection of toy\ndatasets.  One of these is the ever-popular MNIST database (Modified\nNational Institute of Standards and Technology database), a database\nof handwritten digits which is quite the popular target for machine\nlearning courses.  Let&rsquo;s load it.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "source" : [
            "from sklearn import datasets\ndigits = datasets.load_digits()"
         ],
         "metadata" : {},
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "What is this?  The images are available under \\`digits.images\\`.\n\n"
         ],
         "metadata" : {}
      },
      {
         "outputs" : [],
         "source" : [
            "print('{} images'.format(digits.images.shape[0]))"
         ],
         "metadata" : {},
         "cell_type" : "code",
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "Each image is an $8 \\times 8$ array of numbers between 0 and 16,\ninclusive.  Let&rsquo;s use `imshow` to see one of them.\n\n"
         ],
         "metadata" : {}
      },
      {
         "outputs" : [],
         "execution_count" : 1,
         "metadata" : {},
         "source" : [
            "%matplotlib inline\nimport matplotlib.pyplot as plt\ndef show_image(i):\n    print(\"handwritten digit {}\".format( digits.target[i] ) )\n    plt.imshow(digits.images[i])\n    plt.show()\nshow_image(17)"
         ],
         "cell_type" : "code"
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "The underlying data is stored in `digits.data`.  Our goal is to learn\nhow to relate the content in `digits.data` to the labels in\n`digits.target`.\n\n****Warning:**** This data is highly preprocessed to center the\nhandwritten digits, threshold the bitmaps, etc.  This makes it a\npopular dataset when practicing machine learning algorithms, precisely\nbecause it isn&rsquo;t particularly painful.\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "## Some data for a supervised classification task\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "metadata" : {},
         "source" : [
            "We conventionally use `X` and `y` for the data and the labels.  We also want to split into a testing set and a training set.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "from sklearn.model_selection import train_test_split \n\nX = digits.data\ny = digits.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify=y)"
         ],
         "cell_type" : "code",
         "metadata" : {},
         "execution_count" : 1,
         "outputs" : []
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "It&rsquo;s hilariously easy to set up a $k$-nearest neighbors classifier.\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "cell_type" : "code",
         "source" : [
            "from sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nprint(knn.score(X_test, y_test))"
         ],
         "metadata" : {},
         "outputs" : []
      },
      {
         "metadata" : {},
         "source" : [
            "In this case, `KNeighborsClassifier` is calculating the distance to\nnearby images, finding the $k$ nearest, and those $k$ neighbors vote\nfor the label.  Note that `fit` essentially takes no time, because all\n`fit` does is store all the training data.  This results in &ldquo;large&rdquo;\nmodels that require quite a bit memory.  Moreover, `predict` is slow\nif there are many points, because it is costly to search for nearby\nneighbors.\n\nWe use `sklearn.metrics` to understand how well our model works.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "source" : [
            "from sklearn import metrics\npredicted = knn.predict(X_test)\nmetrics.classification_report(y_test, predicted)"
         ],
         "metadata" : {},
         "execution_count" : 1
      },
      {
         "metadata" : {},
         "source" : [
            "A key idea in classifiers is a [confusion\nmatrix]([https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix)) which helps us\nunderstand if our model is systematically misclassifying certain inputs.\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "outputs" : [],
         "source" : [
            "metrics.confusion_matrix(y_test, predicted)"
         ],
         "metadata" : {},
         "cell_type" : "code",
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## Try other models\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "And `scikit-learn` provides a bunch of other classifiers to choose from.  Like a support vector machine with a linear kernel.\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "source" : [
            "### Support vector machines with linear kernel\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "source" : [
            "from sklearn.svm import SVC\nmodel = SVC(kernel=\"linear\", C=0.025)\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))"
         ],
         "metadata" : {},
         "cell_type" : "code",
         "outputs" : []
      },
      {
         "source" : [
            "### Support vector machines with radial basis functions\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code",
         "source" : [
            "from sklearn.svm import SVC\nmodel = SVC(gamma=0.05, C=5)\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "### Decision trees\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "outputs" : [],
         "execution_count" : 1,
         "cell_type" : "code",
         "source" : [
            "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=10)\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))"
         ],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "### Logistic regression\n\n"
         ],
         "metadata" : {}
      },
      {
         "outputs" : [],
         "source" : [
            "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='lbfgs', max_iter=10000)\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))"
         ],
         "cell_type" : "code",
         "metadata" : {},
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "### Random forests\n\n"
         ],
         "metadata" : {}
      },
      {
         "outputs" : [],
         "cell_type" : "code",
         "source" : [
            "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nmodel = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))"
         ],
         "metadata" : {},
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "## How many neighbors should we use?\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "source" : [
            "scores = {}\n\nfor k in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    scores[k] = knn.score(X_test, y_test)\n\nplt.plot(scores.keys(), scores.values())\nplt.xlabel('k')\nplt.ylabel('score')\nplt.show()"
         ],
         "cell_type" : "code",
         "metadata" : {},
         "outputs" : []
      },
      {
         "source" : [
            "## Homework\n\n"
         ],
         "cell_type" : "markdown",
         "metadata" : {}
      },
      {
         "metadata" : {},
         "source" : [
            "Instead of making a single test/train split, use\n`sklearn.model_selection.KFold` to get a sense of the distribution of\nmodel scores.\n\n"
         ],
         "cell_type" : "markdown"
      }
   ],
   "metadata" : {
      "kernelspec" : {
         "display_name" : "Python 3",
         "name" : "python3",
         "language" : "python"
      },
      "language_info" : {
         "file_extension" : ".py",
         "codemirror_mode" : {
            "version" : 3,
            "name" : "ipython"
         },
         "nbconvert_exporter" : "python",
         "version" : "3.5.2",
         "pygments_lexer" : "ipython3",
         "mimetype" : "text/x-python",
         "name" : "python"
      },
      "org" : null
   },
   "nbformat_minor" : 0,
   "nbformat" : 4
}
