{
   "metadata" : {
      "language_info" : {
         "name" : "python",
         "version" : "3.5.2",
         "codemirror_mode" : {
            "version" : 3,
            "name" : "ipython"
         },
         "pygments_lexer" : "ipython3",
         "mimetype" : "text/x-python",
         "nbconvert_exporter" : "python",
         "file_extension" : ".py"
      },
      "kernelspec" : {
         "name" : "python3",
         "display_name" : "Python 3",
         "language" : "python"
      },
      "org" : null
   },
   "nbformat" : 4,
   "nbformat_minor" : 0,
   "cells" : [
      {
         "cell_type" : "markdown",
         "source" : [
            "The **likelihood** is a probability of the joint occurence of the\nobservations for specified values of the parameters of the model.  In\nshort, it describes how likely we are to see the data assuming a given\nmodel.\n\nThe critical insight is the following: armed with an expression for\nthe likelihood, we can choose the model parameters that are most\nplausible given our observations.  Specifically, we obtain the\nparameter estimates by finding the parameter values that maximize the\nlikelihood function.\n\nLet&rsquo;s try this in order to perform **logistic regression** (something of\na misnomer since we&rsquo;re treating it as a classifier!).\n\n"
         ],
         "metadata" : {}
      },
      {
         "metadata" : {},
         "source" : [
            "## Computing likelihood\n\n"
         ],
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "We can find an expression for the likelihood.\n\nWe have parameters $\\beta_1, \\ldots, \\beta_n$ along with an intercept\n$\\beta_0$, and we define $p(x)$ so that $\\log \\frac{p(x)}{1 - p(x)} = \\sum \\beta_i x_i +\n\\beta_0)$.\n\nSuppose we have $N$ input vectors $x_1, \\ldots, x_N$ which are each\nclassified with a corresponding $y_i \\in \\{ 0, 1 \\}$.  Then the\nlikelihood of making those observations (assuming our model is\ncorrectly estimating the &ldquo;actual&rdquo; probability!) is $\\prod_{i=1}^N\np(x_i)^{y_i} ( 1 - p(x_i) )^{1 - y_i}$.\n\nInstead of maximizing likelihood, we will maximize the log likelihood\nwhich conveniently replaces the product with a sum.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## Some fake data\n\n"
         ]
      },
      {
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "from sklearn.datasets import make_blobs\n\nN = 100\npoints, labels = make_blobs(n_samples=N, centers=2, cluster_std=1, n_features=2)"
         ],
         "outputs" : [],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## The model\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "In this case, there are there parameters to the model.  We&rsquo;ll call\nthem `bx` and `by` and `b0`.  Then for a point $(x,y)$ in the plane,\ndefine $p(x,y) = \\sigma( b_x \\cdot x + b_y \\cdot y + b_0 )$ which is\nthe probability that the point $(x,y)$ receives the $1$ label.\n\n"
         ],
         "metadata" : {}
      },
      {
         "execution_count" : 1,
         "source" : [
            "from scipy.special import expit\n\ndef p(x, y, bx, by, b0):\n    return expit( bx * x + by * y + b0 )"
         ],
         "outputs" : [],
         "metadata" : {},
         "cell_type" : "code"
      },
      {
         "source" : [
            "## The cost function\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "source" : [
            "We want to maximize the likelihood, or alternatively minimize the\nnegative of the log likelihood.  Let&rsquo;s make our cost function be the\nnegative of the log likelihood.\n\n"
         ],
         "metadata" : {},
         "cell_type" : "markdown"
      },
      {
         "cell_type" : "code",
         "outputs" : [],
         "metadata" : {},
         "source" : [
            "def cost(bx, by, b0):\n    result = 0\n    for i in range(N):\n        pp = p( points[i,0], points[i,1], bx, by, b0 )\n        if pp == 1.0 or pp == 0.0:\n            continue\n        if labels[i] == 0:\n            result = result + math.log( 1 - pp )\n        else:\n            result = result + math.log( pp )\n    return -result"
         ],
         "execution_count" : 1
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "## Optimizing\n\n"
         ]
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "There are a number of solvers in `scikit-learn`.\n\nA terrible idea is just to randomly try a bunch of things and see how small we can make things.\n\n"
         ],
         "metadata" : {}
      },
      {
         "source" : [
            "plt.hist( [ cost(*np.random.normal( 0, 2, size=3 )) for _ in range(100)] )\nplt.show()"
         ],
         "execution_count" : 1,
         "outputs" : [],
         "metadata" : {},
         "cell_type" : "code"
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Let&rsquo;s identify one of those low-cost choices.\n\n"
         ]
      },
      {
         "outputs" : [],
         "metadata" : {},
         "execution_count" : 1,
         "source" : [
            "best = [1,2,3]\nbest_cost = cost( *best )\n\nfor _ in range(100):\n    b = np.random.normal( 0, 1, size=3 )\n    if cost(*b) < best_cost:\n        best = b\n        best_cost = cost(*best)\n        print(best_cost)"
         ],
         "cell_type" : "code"
      },
      {
         "cell_type" : "markdown",
         "metadata" : {},
         "source" : [
            "Let&rsquo;s plot it!\n\n"
         ]
      },
      {
         "cell_type" : "code",
         "execution_count" : 1,
         "source" : [
            "import matplotlib.pyplot as plt\n\nx_min, x_max = points[:, 0].min() - .5, points[:, 0].max() + .5\ny_min, y_max = points[:, 1].min() - .5, points[:, 1].max() + .5\nh = 0.1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\ndef predict(point):\n    return p( point[0], point[1], *best) > 0.5\n\nZ = np.array([predict(p) for p in np.c_[xx.ravel(), yy.ravel()]])\n\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter( points[:,0], points[:,1] )\nplt.show()"
         ],
         "outputs" : [],
         "metadata" : {}
      },
      {
         "cell_type" : "markdown",
         "source" : [
            "If you like this sort of thing, you could try using our SGD code to\nattack this problem.\n\n"
         ],
         "metadata" : {}
      }
   ]
}
